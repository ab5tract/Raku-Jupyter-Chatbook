{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM functions and chat objects\n",
    "\n",
    "Anton Antonov   \n",
    "[\"Jupyter::Chatbook\" Raku package at GitHub](https://github.com/antononcube/Raku-Jupyter-Chatbook)   \n",
    "August, September 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook we show how Large Language Models (LLMs) functions and LLM chat objects can be created and used in a notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** For LLM functions and chat objects the functionalities of [\"Jupyter::Kernel\"](https://github.com/bduggan/raku-jupyter-kernel), [BDp1], suffice. In other words, the \"chatbook\" extensions of provided by \"Jupyter::Chatbook\", [AAp1], is not needed.\n",
    "\n",
    "**Remark:** The LLM functions and chat objects are provided by the package [\"LLM::Functions\"](https://raku.land/zef:antononcube/LLM::Functions), [AA1, AAp2], which in turn uses the packages [\"WWW::OpenAI\"](https://raku.land/zef:antononcube/WWW::OpenAI), [AAp3], and [\"WWW::PaLM\"](https://raku.land/zef:antononcube/WWW::PaLM), [AAp4].\n",
    "\n",
    "**Remark:** The API keys for the LLM functions and chat objects are taken from the Operating System (OS) environmental variables `OPENAI_API_KEY` and `PALM_API_KEY`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------ \n",
    "\n",
    "## LLM functions\n",
    "\n",
    "The \"standard\" way to utilize LLMs is with the package \"LLM::Functions\", [AA1, AAp1]. I.e. without using the specific, dedicated packages \"WWW::OpenAI\", [AAp2], and \"WWW::PaLM\", [AAp3].\n",
    "\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "<table>\n",
       "  <tr>\n",
       "    <th>Column 1</th>\n",
       "    <th>Column 2</th>\n",
       "    <th>Column 3</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Row 1, Column 1</td>\n",
       "    <td>Row 1, Column 2</td>\n",
       "    <td>Row 1, Column 3</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Row 2, Column 1</td>\n",
       "    <td>Row 2, Column 2</td>\n",
       "    <td>Row 2, Column 3</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Row 3, Column 1</td>\n",
       "    <td>Row 3, Column 2</td>\n",
       "    <td>Row 3, Column 3</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Row 4, Column 1</td>\n",
       "    <td>Row 4, Column 2</td>\n",
       "    <td>Row 4, Column 3</td>\n",
       "  </tr>\n",
       "</table>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%% html\n",
    "use LLM::Functions;\n",
    "\n",
    "my &ftbl = llm-function({\"The HTML code of a random table with $^a rows and $^b columns is : \"}, e => 'OpenAI');\n",
    "\n",
    "&ftbl(4,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** Since the LLM function above produces (or it is suppossed to produce) HTML code we use the magic spec `%% html`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Number extraction from LLM responses\n",
    "\n",
    "Often LLM return larger number with comma delimiters between the digits. Here is an example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "As of July 2020, the estimated population of Niger is 23,322,921."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my $pop = llm-function()(\"What is the population of Niger?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to extract the numbers from those responses is to use the token `<local-number>` provided by the package [\"Intl::Token::Number\"](https://raku.land/zef:guifa/Intl::Token::Number), [MSp1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(｢2020｣\n",
       " local-number => ｢2020｣ ｢23,322,921.｣\n",
       " local-number => ｢23,322,921.｣)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use Intl::Token::Number;\n",
    "\n",
    "$pop ~~ m:g/ <local-number> /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, a sub-parser from the packatge [\"Text::SubParsers\"](https://raku.land/zef:antononcube/Text::SubParsers), [AAp5], can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$[\"\\n\\nAs of\", DateTime.new(2020,7,1,0,0,0), \", the estimated population of Niger is\", 23322921, \".\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use Text::SubParsers;\n",
    "\n",
    "sub-parser(Whatever).subparse($pop).raku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "## LLM chat objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a chat object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLM::Functions::Chat(chat-id = , llm-evaluator.conf.name = chatpalm, messages.elems = 0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my $chatObj = llm-chat(\n",
    "    conf=>'ChatPaLM', \n",
    "    prompt => 'You are Raku coding instructor. You are the best Raku programmer and you know Raku documentation very well. You answers are concise, mostly with code');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we give the chat object a message, and specify the output to be in Markdown format using the magic spec `%% markdown`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% markdown\n",
    "$chatObj.eval('How do you get the characters of a string?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% markdown\n",
    "$chatObj.eval('How do you make the original word from the characters in the previous example')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Articles\n",
    "\n",
    "[AA1] Anton Antonov,\n",
    "[\"Workflows with LLM functions\"](https://rakuforprediction.wordpress.com/2023/08/01/workflows-with-llm-functions/),\n",
    "(2023),\n",
    "[RakuForPrediction at WordPress](https://rakuforprediction.wordpress.com).\n",
    "\n",
    "[AA2] Anton Antonov,\n",
    "[\"Number guessing games: PaLM vs ChatGPT\"](https://rakuforprediction.wordpress.com/2023/08/06/number-guessing-games-palm-vs-chatgpt/),\n",
    "(2023),\n",
    "[RakuForPrediction at WordPress](https://rakuforprediction.wordpress.com).\n",
    "\n",
    "### Packages\n",
    "\n",
    "[AAp1] Anton Antonov,\n",
    "[Jupyter::Chatbook Raku package](https://github.com/antononcube/Raku-Jupyter-Chatbook),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp2] Anton Antonov,\n",
    "[LLM::Functions Raku package](https://github.com/antononcube/Raku-LLM-Functions),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp3] Anton Antonov,\n",
    "[WWW::OpenAI Raku package](https://github.com/antononcube/Raku-WWW-OpenAI),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp4] Anton Antonov,\n",
    "[WWW::PaLM Raku package](https://github.com/antononcube/Raku-WWW-PaLM),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[AAp5] Anton Antonov,\n",
    "[Text::SubParsers Raku package](https://github.com/antononcube/Raku-Text-SubParsers),\n",
    "(2023),\n",
    "[GitHub/antononcube](https://github.com/antononcube).\n",
    "\n",
    "[BDp1] Brian Duggan,\n",
    "[Jupyter:Kernel Raku package](https://github.com/bduggan/raku-jupyter-kernel),\n",
    "(2017-2023),\n",
    "[GitHub/bduggan](https://github.com/bduggan).\n",
    "\n",
    "[MSp1] Matthew Stuckwisch,\n",
    "[Intl::Token::Number Raku package](https://github.com/alabamenhu/IntlTokenNumber)\n",
    "(2021-2023),\n",
    "[GitHub/alabamenhu](https://github.com/alabamenhu)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Raku",
   "language": "raku",
   "name": "raku"
  },
  "language_info": {
   "file_extension": ".raku",
   "mimetype": "text/plain",
   "name": "raku",
   "version": "6.d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
